# Session Evals

Learn from your AI sessions by generating structured evaluations.

---

## ğŸ’» Use Case

**Important:** Generate and annotate your first eval in 60 seconds.

1. After completing a task with your AI assistant, run:
   ```
   "Generate an eval from my recent session"
   ```

2. Review what was captured:
   ```
   "List my evals"
   ```

3. Add your judgement:
   ```
   "Annotate the latest eval as success with notes: completed refactor cleanly"
   ```

**Stretch goal:** Review pending evals at the end of the week to spot patterns.

**Super-stretch goal:** Use eval insights to update your AGENTS.md instructions.

---

## ğŸ§° Primitive

### The Problem

You complete work with AI assistants but have no systematic way to:
- Track what worked vs. what didn't
- Identify patterns in AI behavior
- Learn from past sessions to improve future ones
- Build an institutional memory of AI interactions

### The Solution

Session evals capture structured snapshots of your AI sessions:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Claude Code Session                    â”‚
â”‚  - User prompts                         â”‚
â”‚  - Tool calls made                      â”‚
â”‚  - Outcomes achieved                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  generate_eval                          â”‚
â”‚  - Parses session trace                 â”‚
â”‚  - Extracts key patterns                â”‚
â”‚  - Creates markdown eval file           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Eval File (core/evals/)                â”‚
â”‚  - Session metadata                     â”‚
â”‚  - Conversation flow                    â”‚
â”‚  - Tool usage summary                   â”‚
â”‚  - AI analysis + suggestions            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  annotate_eval                          â”‚
â”‚  - Your judgement (success/partial/fail)â”‚
â”‚  - Notes on what worked                 â”‚
â”‚  - Lessons for future sessions          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

Evals create a feedback loop. Without them, you repeat the same mistakes. With them, you:

- **Learn**: See which prompts lead to better outcomes
- **Improve**: Update AGENTS.md based on patterns
- **Track**: Measure AI effectiveness over time
- **Debug**: Understand why sessions go sideways

---

## Tools Available

| Tool | Purpose | Key Parameters |
|------|---------|----------------|
| `generate_eval` | Create eval from session | `session_id` (or `'recent'`) |
| `list_evals` | View all evals | `limit`, `judgement` filter |
| `annotate_eval` | Add judgement/notes | `eval_file`, `judgement`, `annotation` |
| `get_eval_summary` | Aggregate stats | none |

### Judgement Values

| Value | When to Use |
|-------|-------------|
| `success` | Task completed as expected |
| `partial` | Some progress, but gaps remain |
| `failure` | Task not completed or went wrong |
| `pending` | Not yet reviewed (default) |

---

## Anatomy of an Eval

Each eval is a markdown file with YAML frontmatter:

```yaml
---
session_id: abc123
project: product-os
timestamp: 2025-01-15T10:30:00Z
model: claude-opus-4-5-20251101
message_count: 12
tool_call_count: 8
judgement: pending
annotation: ""
axial_codes: ['efficient-tool-use', 'clear-communication']
reviewed: false
---

# Session Eval: abc123

## User Intent
What the user asked for

## Conversation Flow
Turn-by-turn summary

## Tool Usage Summary
Table of tools and counts

## AI Analysis
- Suggested judgement
- Detected patterns
- Improvement suggestions
```

---

## Eval Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Complete a session                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Was it        â”‚
        â”‚ significant?  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           Yes  â”‚  No â†’ Skip
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generate_eval session_id='recent'       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Review the eval file                    â”‚
â”‚ - Does the AI analysis match reality?   â”‚
â”‚ - What actually happened?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ annotate_eval with your judgement       â”‚
â”‚ - success / partial / failure           â”‚
â”‚ - Notes on what worked or didn't        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Pattern       â”‚
        â”‚ emerging?     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           Yes  â”‚  No â†’ Done
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update AGENTS.md with learnings         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tips

**When to generate evals:**
- After completing significant tasks
- When something went unexpectedly well or poorly
- At the end of focused work sessions

**What makes a good annotation:**
- Specific ("file search was slow") not vague ("could be better")
- Actionable ("add glob pattern hint to AGENTS.md")
- Honest (mark failures as failures to learn from them)

**Weekly review ritual:**
```
"Show me pending evals"
"What patterns do you see across my recent evals?"
```

---

## Common Patterns to Look For

| Pattern | Signal | Action |
|---------|--------|--------|
| Repeated clarification | Prompts are unclear | Improve AGENTS.md examples |
| Tool call failures | Missing context | Add tool usage hints |
| Successful shortcuts | Agent found efficiency | Document in workflows |
| Consistent partial | Scope too large | Break tasks smaller |

---

Back to: [Tutorials Home](README.md)
